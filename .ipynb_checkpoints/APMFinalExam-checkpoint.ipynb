{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\" ><i style=\"font-size:larger\"> Analytics and Predictive Modeling </i> <br/> Final Exam Assignment </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the tasks in this notebook.  **SAVE FREQUENTLY!!!!**  Then download and submit the entire notebook. \n",
    "\n",
    "We'll need the following imports throughout this final assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas import read_csv, DataFrame, crosstab\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_recall_curve\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import neurolab\n",
    "\n",
    "ln = log  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, let's load the assignment routines and data for this unit.  Enter your username and then execute the line \n",
    "\n",
    "**%run -i FinalExam.py**\n",
    "\n",
    "<b style=\"font-size:larger\">NOTE:</b> This notebook was zipped along with <b>HeartData.csv, leukemia.csv, </b> and <b>FinalExam.py</b>.  They must be uploaded into the same folder as this notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "username = \"knisleyj\" #\"Replace with Your Username\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run -i FinalExam.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Review/Comparison of the Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic model accomplishes much of what we hoped for -- a linear classifier based on maximum entropy. If the data is separable, then it is -- not surprisingly -- the best in some sense (i.e., with respect to maximum entropy modeling).  \n",
    "\n",
    "If the data is not linearly separable -- or if we don't know whether or not the data is linearly separable -- then neural networks have the feature that they will reduce to linear if the underlying data truly is linearly separable but will provide a highly reliable classifier otherwise.  \n",
    "\n",
    "We will also compare to two other methods that do not require linear separability of the original data.\n",
    "\n",
    "  * **Support Vector Machines:** Data is first embedded into a high dimensional space and then a linear classifier is applied. \n",
    "        \n",
    "  * **Random Forests:** A bootstrap resampling of randomly generated decision trees -- that is, an \"ensemble\" of randomly generated decision trees that via sampling with replacement is used to imply a single classifier.  \n",
    "\n",
    "Let's get started.  You will need to complete each **task** in this notebook.  \n",
    "\n",
    "## Revisiting the Iris\n",
    "\n",
    "We've seen Fisher's classic Iris data before, but let's look at it again -- this time with an array of classifiers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = array( [iris.data[:,0], iris.data[:,2]] ).T  # Sepal Length and Petal Length\n",
    "Y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we look at Logistic Regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(C=1e5)\n",
    "\n",
    "classifier.fit(X, Y)\n",
    "\n",
    "DecisionBoundary(X,classifier.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we look at a Support Vector Machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SVMclassifier = SVC()\n",
    "\n",
    "SVMclassifier.fit(X,Y)\n",
    "\n",
    "DecisionBoundary(X,SVMclassifier.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1: (answer in the space below)**  Why did the Support vector machine not produce linear decision boundaries? "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(answer should replace this text and should include something meaningful about SVM's)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, two instances of a random forest -- just so you will see how they still have the same type of instability that decision trees have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RFclassifier = RandomForestClassifier()\n",
    "\n",
    "RFclassifier.fit(X,Y)\n",
    "\n",
    "DecisionBoundary(X,RFclassifier.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RFclassifier = RandomForestClassifier()\n",
    "\n",
    "RFclassifier.fit(X,Y)\n",
    "\n",
    "DecisionBoundary(X,RFclassifier.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's not forget about the nearest neighbor classifiers!  They are great, can be rewired, and have fantastic modeling properties. \n",
    "\n",
    "But they lack the theory necessary to manage them (i.e., if we knew more about the types of networks that nearest neighbor methods are producing, then nearest neighbor methods might have been the only things we looked at!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kNNclassifier = KNeighborsClassifier( n_neighbors = 1)\n",
    "\n",
    "kNNclassifier.fit(X,Y)\n",
    "\n",
    "DecisionBoundary(X,kNNclassifier.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:** What happens as *k* becomes larger?  A better classifier?? or worse?? Or about the same??  Closer to linear?? "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Answer here in complete sentences (replace this text).  Graded based on how well your answer describes what happens as k becomes larger. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's look at the neural networks.  \n",
    "\n",
    "<b style=\"font-size:larger\">NOTE: Pybrain or Keros or Caffe or Torch are the preferred libraries, but they have a significant learning curve.  I chose neurolab because it is easy to learn quickly (but not very strong)\n",
    "<h4 align='center'>AND</h4>\n",
    "<br/>\n",
    "because it allows the Vanishing Gradient problem (I.E., YOU WILL ALMOST CERTAINLY HAVE TO RESTART SOME CELLS IN ORDER TO EVENTUALLY GET RESULTS</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ANN = neurolab.net.newff([[4,8],[1,7]], [2,5,3])\n",
    "ANN.trainf = neurolab.train.train_cg "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our output layer has 3 neurons -- one for class 0, one for class 1, and one for class 2.  Thus, an output like [1,0,0]  implies class 0, while an output [0,0,1]  implies class 2.  \n",
    "\n",
    "The procedures below translate the data to/from this 3 output node arrangement.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y4ANN = zeros( (len(Y),3) )\n",
    "for i in range(len(Y)):\n",
    "    Y4ANN[i,Y[i]] = 1\n",
    "    \n",
    "def ANNpred(Data):\n",
    "    Results = ANN.sim(Data)\n",
    "    nrw,ncl = Results.shape\n",
    "    pred = zeros(nrw)\n",
    "    for i in range(nrw):\n",
    "        for j in range(3):\n",
    "            if( Results[i,j] > 0.5):\n",
    "                pred[i] = j\n",
    "                break\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "May need to repeat the next cell several times. \n",
    "\n",
    "<b style=\"font-size:larger\">NOTE: If training stops suddenly or stops on an error well above 0, then it is because of the vanishing gradient problem. For now, simply run the cell below again.  May have to do so several times. </b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ANN.init() \n",
    "err = []\n",
    "\n",
    "err.extend(  ANN.train(X, Y4ANN, show = 50, epochs=2000 ) )\n",
    "\n",
    "plot(err)\n",
    "ylim(0,max(err))\n",
    "xlabel('Epoch number')\n",
    "ylabel('error (default SSE)')\n",
    "err[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DecisionBoundary(X,ANNpred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network will (eventually) be the best, but it will also take the most work to get there. \n",
    "\n",
    "**Question 3:** Which of the classifiers above does the ANN most closely resemble? "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Answer here (should replace this text and should include justification for why you have this answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>&nbsp;</p>\n",
    "\n",
    "## Heart Disease Diagnostic Dataset -- Disease versus No Disease\n",
    "\n",
    "Here we look at a diagnostic test for heart disease.  There are 11 variables,  \n",
    "\n",
    " * age, sex, cp=chest pain, restbps = resting blood pressure, chol = cholestoral in mg/dl, fbs = fasting blood sugur, restecg (0=normal, 1 = ST-T abnormality, 2 = Probable ventricular hypertrophy), thalach = max heart rate, exang = exercise induced angina, thal=thalium heart scan, and num = class of the heart disease\n",
    "\n",
    "Let's load the data and take a look. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HeartDisease = loadtxt('HeartDisease.csv', delimiter=\",\" )\n",
    "DataFrame(HeartDisease, columns=['age', 'sex', 'cp', 'restbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'num']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Immediately, we see a problem.  Some columns are only 0 and 1, while other columns are numerical.  \n",
    "\n",
    "For a linear classifier (no embedding into higher dimensions), this is not an issue because the coefficients can do the appropriate scaling, as we shall soon see.   \n",
    "\n",
    "First however, let's split the data into Training and Testing sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TrainingPatterns, TestingPatterns, TrainingTargets, TestingTargets = \\\n",
    "train_test_split(HeartDisease[:,:10],HeartDisease[:,10], test_size = 0.2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(C=1000)\n",
    "\n",
    "classifier.fit(TrainingPatterns,TrainingTargets)\n",
    "\n",
    "\n",
    "Predictions = classifier.predict(TestingPatterns)\n",
    "Predictions\n",
    "\n",
    "fpr, tpr, threshholds = roc_curve(TestingTargets, Predictions)\n",
    "\n",
    "ROCplot(fpr,tpr, auc = True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, anything which addresses nonlinearity is going to be corrupted by the relative sizes of the individual features. \n",
    "\n",
    "For example, a support vector machine should perform at least as well or better than logistic regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SVMclassifier = SVC()\n",
    "\n",
    "SVMclassifier.fit(TrainingPatterns,TrainingTargets)\n",
    "\n",
    "Predictions = SVMclassifier.predict( TestingPatterns)\n",
    "Predictions\n",
    "\n",
    "fpr, tpr, threshholds = roc_curve(TestingTargets, Predictions)\n",
    "auc( fpr, tpr)\n",
    "\n",
    "ROCplot(fpr, tpr, True, threshholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The poor performance is because of the relative magnitudes of the data set.  \n",
    "\n",
    "Thus, we first employ a **preprocessing** step.  In particular, we scale the data so that each feature (column) is between 0 and 1, inclusive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "HeartDiseaseRescaled = min_max_scaler.fit_transform( HeartDisease )\n",
    "View(HeartDiseaseRescaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We re-partition the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TrainingPatterns, TestingPatterns, TrainingTargets, TestingTargets = \\\n",
    "train_test_split(HeartDiseaseRescaled[:,:10],HeartDiseaseRescaled[:,10], test_size = 0.2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4:** How are the **TrainingPatterns** just produced any different than the ones earlier? "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Answer here (replace this text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start -- again -- with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(C=1000)\n",
    "\n",
    "classifier.fit(TrainingPatterns,TrainingTargets)\n",
    "\n",
    "\n",
    "Predictions = classifier.predict(TestingPatterns)\n",
    "Predictions\n",
    "\n",
    "fpr, tpr, threshholds = roc_curve(TestingTargets, Predictions)\n",
    "\n",
    "ROCplot(fpr,tpr, True, threshholds )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the support vector machine again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SVMclassifier = SVC()\n",
    "\n",
    "SVMclassifier.fit(TrainingPatterns,TrainingTargets)\n",
    "\n",
    "Predictions = SVMclassifier.predict( TestingPatterns)\n",
    "Predictions\n",
    "\n",
    "fpr, tpr, threshholds = roc_curve(TestingTargets, Predictions)\n",
    "auc( fpr, tpr)\n",
    "\n",
    "ROCplot(fpr, tpr, True, threshholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the random forest... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RFclassifier = RandomForestClassifier()\n",
    "\n",
    "RFclassifier.fit(TrainingPatterns,TrainingTargets)\n",
    "\n",
    "Predictions = RFclassifier.predict( TestingPatterns)\n",
    "Predictions\n",
    "\n",
    "fpr, tpr, threshholds = roc_curve(TestingTargets, Predictions)\n",
    "auc( fpr, tpr)\n",
    "\n",
    "ROCplot(fpr, tpr, True, threshholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we finish up with a neural network -- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ANN = neurolab.net.newff([[0,1] for i in range(10)], [10, 10, 1])\n",
    "ANN.trainf = neurolab.train.train_cg\n",
    "ANN.init()\n",
    "err = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not sure how many times you will need to run the command below, but only 2 or 3 should be plenty. However, <b style=\"font-size:larger\">if the vanishing gradient problem shows up,</b> you will have to reinitialize the network (cell above) and then execute this again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "err.extend( ANN.train(TrainingPatterns, TrainingTargets.reshape( (len(TrainingTargets),1) ), show = 50 ) )\n",
    "\n",
    "plot(err)\n",
    "ylim(0,max(err))\n",
    "xlabel('Epoch number')\n",
    "ylabel('error (default SSE)')\n",
    "err[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Predictions = ANN.sim(TestingPatterns)\n",
    "\n",
    "fpr, tpr, threshholds = roc_curve(TestingTargets, Predictions)\n",
    "auc( fpr, tpr)\n",
    "\n",
    "ROCplot(fpr, tpr, True, threshholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5:** What is the ROC curve above telling us?  Comment about (a) what part of the data it is applied to, (b) the area under the curve, and (c) (VERY IMPORTANT) the value of \"Closest point: parameter value\", because it indicates overfitting in some of these classifiers (Why?).  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Answer here (replace this text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>&nbsp;</p>\n",
    "\n",
    "\n",
    "## The Leukemia Microarray Data set\n",
    "\n",
    "Suppose now that we look at a fairly typical application of Machine learning in Big Data -- that of studying the genetic causes of disease.  Our goal in this case is \n",
    "\n",
    "**feature detection:** Determining which features (genes) are most responsible for a macroscopic phenomenon (e.g., disease).  Samples (biopsies) are collected from both a disease-free control group and a disease-expressing experimental group.  The samples are used to produce *micro-arrays*, which are arrays that measure how much a gene is being expressed in the sample (e.g., measuring relative amounts of RNA corresponding to the DNA of the gene).  \n",
    "    \n",
    "The data we will look at is for leukemia. There are activity levels for 7129 genes from a total of 72 samples.  Moreover, the data has been preprocessed so that each genes' activity has a mean of 0 and a standard deviation of 1 across the entire experimental and control groups.  This is known as \"Standardization\" of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "StandardizedLeukemiaData   = loadtxt('leukemiaALL.csv',delimiter = \",\")\n",
    "\n",
    "TrainingPatterns, TestingPatterns, TrainingTargets, TestingTargets = \\\n",
    "train_test_split(StandardizedLeukemiaData[:,1:],StandardizedLeukemiaData[:,0], test_size = 0.2 )\n",
    "\n",
    "NumTrain, NumberOfGenes = TrainingPatterns.shape\n",
    "NumTest , NumberOfGenes =  TestingPatterns.shape\n",
    "NumberOfGenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some plots of individual samples genetic expressions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot( TrainingPatterns[0] )\n",
    "xlabel( 'Gene' )\n",
    "ylabel( 'Normalized activation' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot( TrainingPatterns[1] )\n",
    "xlabel( 'Gene' )\n",
    "ylabel( 'Normalized activation' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot( TrainingPatterns[2] )\n",
    "xlabel( 'Gene' )\n",
    "ylabel( 'Normalized activation' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression is the workhorse!! We always start with it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(C=1000)\n",
    "\n",
    "classifier.fit(TrainingPatterns,TrainingTargets)\n",
    "\n",
    "\n",
    "Predictions = classifier.predict(TestingPatterns)\n",
    "\n",
    "fpr, tpr, threshholds = roc_curve(TestingTargets, Predictions)\n",
    "\n",
    "ROCplot(fpr, tpr, True, threshholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, linear in a sufficiently high dimensional context has great potential because of the massive independence of genes from one another -- a gene responsible for metabolism is relatively independent of a gene responsible for eye color, for example.  \n",
    "\n",
    "So let's try out an SVM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SVMclassifier = SVC( )\n",
    "\n",
    "SVMclassifier.fit(TrainingPatterns,TrainingTargets)\n",
    "\n",
    "Predictions = SVMclassifier.predict( TestingPatterns)\n",
    "\n",
    "fpr, tpr, threshholds = roc_curve(TestingTargets, Predictions)\n",
    "\n",
    "ROCplot(fpr, tpr, True, threshholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a random forest, if only for completeness. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RFclassifier = RandomForestClassifier()\n",
    "\n",
    "RFclassifier.fit(TrainingPatterns,TrainingTargets)\n",
    "\n",
    "Predictions = RFclassifier.predict( TestingPatterns)\n",
    "Predictions\n",
    "\n",
    "fpr, tpr, threshholds = roc_curve(TestingTargets, Predictions)\n",
    "\n",
    "ROCplot(fpr, tpr, True, threshholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, the standardization of the data -- while highly necessary -- produces features with different magnitude.  We can reduce the impact of the standardization by using an SVM with a **<i>linear kernel</i>**, because linearity allows rescaling of the coefficients to compensate for different scales of the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SVMclassifier = SVC( kernel = 'linear' )\n",
    "\n",
    "SVMclassifier.fit(TrainingPatterns,TrainingTargets)\n",
    "\n",
    "Predictions = SVMclassifier.predict( TestingPatterns)\n",
    "\n",
    "fpr, tpr, threshholds = roc_curve(TestingTargets, Predictions)\n",
    "\n",
    "ROCplot(fpr, tpr, True, threshholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, we do not have many observations, so the ROC and AUC can vary greatly from one random training - testing subdivision to the next.  Consequently, we need to use other metrics as well. For example, here is the **<i>precision-recall curve</i>**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "precision, recall, threshold = precision_recall_curve(TestingTargets, Predictions)\n",
    "PRCplot(recall, precision, auc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The support vector machine is not only probably the best for this example, but it also gives us a bonus -- Support Vectors!!.  We label these as **SupportData**, which we can obtain from either the property **support_vectors_** of our classifier object, or from the indices corresponding to the support vectors, which are the property **support_**  of the classifier object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SupportIndices = SVMclassifier.support_\n",
    "SupportData    = TrainingPatterns[ SVMclassifier.support_ ]\n",
    "SupportTargets = TrainingTargets[  SVMclassifier.support_ ]\n",
    "\n",
    "SupportIndices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The support vectors can also tell us which *features* (i.e., genes) are most responsible for leukemia.  This is known as __Feature Selection__ , which is the selection of the features most relevant to the phenomenon (e.g., leukemia ) that produced the data. \n",
    "\n",
    "For example, the command <b>LinearSVC</b> is equivalent to **SVC** with **kernel = linear,** but it is implemented differently for computational reasons.  However, as **kernel = linear** indicates, we can use this classifier to select which features are most responsible for leukemia.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FeatureSelector = LinearSVC(C=1000, penalty=\"l1\", dual=False)\n",
    "Reduced = FeatureSelector.fit_transform(SupportData, SupportTargets )\n",
    "\n",
    "FeatureSelector.predict( TestingPatterns)\n",
    "\n",
    "fpr, tpr, threshholds = roc_curve(TestingTargets, Predictions)\n",
    "\n",
    "print(\"\\n\\n       Leukemia predictions restricted to \\n       those genes identified by FeatureSelector\")\n",
    "ROCplot(fpr, tpr, True, threshholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6:** What does this final ROC plot show -- be specific, and talk about genes! "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Answer here (replace this text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correspondingly, support vector machines are often used as a preprocessing step for **<i>dimensionality reduction</i>** by first choosing only those features that are most likely to contribute to the classifier.  \n",
    "\n",
    "In fact, we have avoided neural networks to this point with the leukemia data because the memory and processing requirements are beyond what a free Wakari account can handle.  However, if we use feature selection to reduce the dimensionality of the data, then this is no longer such a big issue.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FeatureSelector = LinearSVC(C=1000, penalty=\"l1\", dual=False)\n",
    "Reduced = FeatureSelector.fit_transform(StandardizedLeukemiaData[:,1:], StandardizedLeukemiaData[:,0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TrainingPatterns, TestingPatterns, TrainingTargets, TestingTargets = \\\n",
    "train_test_split(Reduced,StandardizedLeukemiaData[:,0], test_size = 0.2 )\n",
    "\n",
    "NumTrain, NumberOfGenes = TrainingPatterns.shape\n",
    "NumTest , NumberOfGenes =  TestingPatterns.shape\n",
    "NumberOfGenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ANN = neurolab.net.newff([[-10,10] for i in range(TrainingPatterns.shape[1])], [TrainingPatterns.shape[1],TrainingPatterns.shape[1], 1])\n",
    "ANN.trainf = neurolab.train.train_cg\n",
    "ANN.init()\n",
    "err = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "err.extend( ANN.train(TrainingPatterns, TrainingTargets.reshape( (len(TrainingTargets),1) ), show = 50, epochs = 1000 ) )\n",
    "\n",
    "plot(err)\n",
    "ylim(0,max(err))\n",
    "xlabel('Epoch number')\n",
    "ylabel('error (default SSE)')\n",
    "err[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should notice that the Neural Network converges faster with the dimensionality-reduced data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Predictions = ANN.sim(TestingPatterns)\n",
    "\n",
    "fpr, tpr, threshholds = roc_curve(TestingTargets, Predictions)\n",
    "auc( fpr, tpr)\n",
    "\n",
    "ROCplot(fpr, tpr, True, threshholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digits Recognition\n",
    "\n",
    "Neural networks were -- and often still are -- at the forefront of handwriting and speech recognition.  In particular, neural networks are natural for multiclass data, which means that in practice an output can be assigned to each letter, word, phrase, and so on. \n",
    "\n",
    "However, there are better algorithms today, which we now explore. \n",
    "\n",
    "We are going to explore a simple such application -- that of recognizing hand-written digits 0,...,9.  The data is built into <b>sklearn</b>, but it comes from several thousand samples of hand written digits as recorded in 32 x 32 bit images.  \n",
    "\n",
    "For dimensionality reduction purposes, the 32x32 images are reduced to 8x8 matrices whose coefficients are the total number of \"on\" pixels in the corresponding 4x4 block.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the data!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "digits.images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gray()\n",
    "matshow( digits.images[2] )\n",
    "digits.target[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gray()\n",
    "matshow( digits.images[40] )\n",
    "digits.target[40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we divide the data into testing and training sets: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TrainingPatterns, TestingPatterns, TrainingTargets, TestingTargets = \\\n",
    "train_test_split(digits.data,digits.target, test_size = 0.2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train a Support Vector Machine to recognize digits: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DigitsClassifier = SVC(gamma=0.001)\n",
    "\n",
    "DigitsClassifier.fit( TrainingPatterns, TrainingTargets)\n",
    "\n",
    "Predictions = DigitsClassifier.predict( TestingPatterns )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how do we know how well we've done?  We can in fact produce an entire report for the classifier.  To do so, we use additional procedures in the **metrics** module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(metrics.classification_report(TestingTargets, Predictions)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metrics.adjusted_mutual_info_score(TestingTargets,Predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out.  Execute the next cell and Enter a digit.  \n",
    "\n",
    "* __START WITH THE SHIFT KEY DOWN (Shift key toggles between black and white)__\n",
    "\n",
    "* __Be sure to make the digit extend from the top of the square to the bottom (but not necessary on the sides)__\n",
    "\n",
    "* __Make sure you have good, solid lines!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "EnterADigit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the cells below to see if the Support Vector Machine can read your writing!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ReducedData = ReduceDimensionality(DigitOutputArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gray()\n",
    "matshow(ReducedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"Prediction(s)\", DigitsClassifier.predict( ReducedData.flatten() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last cell contains the digit it thinks you wrote.  Was it right? \n",
    "\n",
    "**Question 7:** Convert this digits example to a *k* Nearest Neighbor approach. Is the result better or worse than a support vector machine? "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The answer should be here, and the classifier cell should be converted to a kNN classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized XOR\n",
    "\n",
    "Finally, let's look at another example -- the generalized XOR.   If **x** is a vector of length *n* containing only 1s and 0s, then we say it is in class \n",
    "\n",
    "sum(**x**) mod 2\n",
    "\n",
    "That is, we sum the coefficients modulo 2, thus producing either class 0 or class 1. This generalizes the ordinary XOR. \n",
    "\n",
    "Below, we use a neural network to implement the generalized **XORdata** given its **XORtarget**. \n",
    "\n",
    "**Note:** The word *implement* means that we use all the data as the training set and see how well we can *implement* the pattern in the data into a neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ANN = neurolab.net.newff([[0,1] for i in range(5)], [5, 20, 1])\n",
    "ANN.trainf = neurolab.train.train_cg\n",
    "ANN.init()\n",
    "err = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "err.extend( ANN.train(XORdata, XORtarget.reshape( (len(XORtarget),1) ), show = 50, epochs = 1000 ) )\n",
    "\n",
    "plot(err)\n",
    "ylim(0,max(err))\n",
    "xlabel('Epoch number')\n",
    "ylabel('error (default SSE)')\n",
    "err[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Predictions = ANN.sim(XORdata)\n",
    "\n",
    "fpr, tpr, threshholds = roc_curve(XORtarget, Predictions)\n",
    "auc( fpr, tpr)\n",
    "\n",
    "ROCplot(fpr, tpr, True, threshholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "precision, recall, threshold = precision_recall_curve(XORtarget, Predictions)\n",
    "PRCplot(recall, precision, auc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8:** How well does Logistic Regression handle the generalized XOR?  __Set up__ a Logistic Classifier that (attempts) to implement the generalized XOR (no testing set -- everything is training) and indicate how well it does using metrics.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higgs Boson Data\n",
    "\n",
    "The next data set is 2500 lines of data that physicists at the Large Hadron Collider (LHC) used to develop methods for detecting if a Higgs Boson was present in a collision.  It is simulated data, but the actual Higgs events are very close to the simulated Higgs events in this data.  (__NOTE:__ This is data set after I imputed missing values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HiggsData = read_csv('HiggsTrainingValsSample.csv')\n",
    "\n",
    "HiggsData.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target is the __Label__,  which is 1 if there is a Higgs boson in the simulated LHC collision event, and a 0 if there is no Higgs in the simulation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HiggsData.Label.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is admittedly a development set.  The original data is over 100 megabytes (about 100 times larger), much too large to use as a pandas __DataFrame__.  However, you could use the data as a matrix -- the __HiggsMatrix__ contains this (execute next cell). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HiggsMatrix = HiggsData.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 9 - 10:** Apply at least 2 algorithms to the Higgs data -- one of which must be a random forest -- and use metrics and the AUC-ROC to tell me which of the two is the better approach for this data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've answered these 10 questions, save your work, download this notebooks, and then upload to D2L (elearn.etsu.edu).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__BONUS:__  An automatic \"A\" for the course if you can develop a classifier for the Midterm Problem based on the MovieLens data that validates 5 times with an AUC over 0.92 (i.e., if the classifier gets an \"A\", you do to!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "name": "APMFinalExam.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
